{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 8 files: 100%|██████████| 8/8 [00:14<00:00,  1.75s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:02<00:00,  3.35it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096, padding_idx=2)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): MistralRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load reasoning model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "reasoning_model_id = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "cache_dir = 'model_cache/'\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "reasoning_tokenizer = AutoTokenizer.from_pretrained(reasoning_model_id, use_fast=True)\n",
    "reasoning_model = AutoModelForCausalLM.from_pretrained(\n",
    "    reasoning_model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    cache_dir = cache_dir\n",
    ")\n",
    "reasoning_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DebertaV2ForSequenceClassification(\n",
       "  (deberta): DebertaV2Model(\n",
       "    (embeddings): DebertaV2Embeddings(\n",
       "      (word_embeddings): Embedding(128100, 1024, padding_idx=0)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): DebertaV2Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (pos_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (rel_embeddings): Embedding(512, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (pooler): ContextPooler(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (dropout): Dropout(p=0, inplace=False)\n",
       "  )\n",
       "  (classifier): Linear(in_features=1024, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get Process Reward Model\n",
    "# (PRM approximation)\n",
    "reward_model_id = \"OpenAssistant/reward-model-deberta-v3-large\"\n",
    "reward_tokenizer = AutoTokenizer.from_pretrained(reward_model_id)\n",
    "reward_model = AutoModelForSequenceClassification.from_pretrained(reward_model_id).to(\"cuda\")\n",
    "reward_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are using Output Reward Model to calculate the scores for steps. Need to write code to performs stepwise scoring\n",
    "# Simulate stepwise reward using a final-answer reward model\n",
    "def stepwise_prm_score(prompt, trace, reward_model, tokenizer, device=\"cuda\"):\n",
    "    steps = trace.split(\". \")\n",
    "    cumulative_score = 0.0\n",
    "    for i in range(1, len(steps)+1):\n",
    "        partial = prompt + \"\\n\" + \". \".join(steps[:i]) # score from original to step i\n",
    "        inputs = tokenizer(partial, return_tensors=\"pt\", truncation=True).to(device)\n",
    "        with torch.no_grad():\n",
    "            score = reward_model(**inputs).logits[0].item()\n",
    "        cumulative_score += score\n",
    "    return cumulative_score / len(steps) if steps else 0.0 # return avg score for each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beam search execution. N is total number of beams and M is beam width\n",
    "def beam_search_with_prm(prompt, reasoning_model, reasoning_tokenizer,\n",
    "                         reward_model, reward_tokenizer,\n",
    "                         N=4, M=2, max_steps=3):\n",
    "    assert N % M == 0, \"N must be divisible by M\"\n",
    "    device = reasoning_model.device\n",
    "\n",
    "    def format_zephyr_prompt(user_prompt: str) -> str:\n",
    "        return f\"<|system|>\\nYou are a helpful assistant.\\n<|user|>\\n{user_prompt}\\n<|assistant|>\\n\"\n",
    "\n",
    "    formatted_prompt = format_zephyr_prompt(prompt)\n",
    "\n",
    "    graph = nx.DiGraph()\n",
    "    node_counter = 0\n",
    "    beams = []\n",
    "\n",
    "    # Step 0: Initial N completions\n",
    "    input_ids = reasoning_tokenizer(formatted_prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "    outputs = reasoning_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=64,\n",
    "        do_sample=False,\n",
    "        num_beams=N,\n",
    "        temperature=0.9,\n",
    "        num_return_sequences=N,\n",
    "        pad_token_id=reasoning_tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    for i in range(N): # sample N output answers from the model on a reasoning tasks\n",
    "        gen_text = reasoning_tokenizer.decode(outputs[i], skip_special_tokens=True)\n",
    "        completion = gen_text.replace(formatted_prompt, \"\").strip()\n",
    "        score = stepwise_prm_score(prompt, completion, reward_model, reward_tokenizer)\n",
    "        node_id = f\"0-{i}\"\n",
    "        graph.add_node(node_id, label=completion[:40]+\"...\", score=score)\n",
    "        beams.append((gen_text, score, node_id))\n",
    "\n",
    "    # Steps 1 to max_steps\n",
    "    for step in range(1, max_steps + 1):\n",
    "        beams = sorted(beams, key=lambda x: x[1], reverse=True)[:M] # get maximum M beamwidth \n",
    "        candidates = []\n",
    "\n",
    "        for parent_text, _, parent_id in beams:\n",
    "            input_ids = reasoning_tokenizer(parent_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "            children = reasoning_model.generate(\n",
    "                input_ids=input_ids,\n",
    "                max_new_tokens=64,\n",
    "                do_sample=False,\n",
    "                num_beams=(N // M),\n",
    "                num_return_sequences=(N // M),\n",
    "                pad_token_id=reasoning_tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "            for i in range(N // M):\n",
    "                child_text = reasoning_tokenizer.decode(children[i], skip_special_tokens=True)\n",
    "                continuation = child_text.replace(parent_text, \"\").strip()\n",
    "                score = stepwise_prm_score(prompt, continuation, reward_model, reward_tokenizer)\n",
    "                node_id = f\"{step}-{i}-{parent_id}\"\n",
    "                graph.add_node(node_id, label=continuation[:40]+\"...\", score=score)\n",
    "                graph.add_edge(parent_id, node_id)\n",
    "                candidates.append((child_text, score, node_id))\n",
    "\n",
    "        beams = sorted(candidates, key=lambda x: x[1], reverse=True)[:N]\n",
    "\n",
    "\n",
    "    return beams, graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization beam tree\n",
    "def plot_trace_graph_tree_clean(graph, figsize=(14, 8), title=\"Beam Search Tree (PRM-Guided)\"):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.patches as mpatches\n",
    "    import networkx as nx\n",
    "    import textwrap\n",
    "\n",
    "    try:\n",
    "        pos = nx.nx_agraph.graphviz_layout(graph, prog='dot')\n",
    "    except:\n",
    "        pos = nx.spring_layout(graph, seed=42)\n",
    "\n",
    "    scores = nx.get_node_attributes(graph, 'score')\n",
    "    labels = nx.get_node_attributes(graph, 'label')\n",
    "    node_colors = [scores[n] for n in graph.nodes()]\n",
    "    node_order = list(graph.nodes())\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "    # Draw edges\n",
    "    nx.draw_networkx_edges(graph, pos, ax=ax, alpha=0.3)\n",
    "\n",
    "    # Draw nodes with color based on PRM score (no edge color)\n",
    "    for node in node_order:\n",
    "        x, y = pos[node]\n",
    "        score = scores.get(node, 0)\n",
    "        fill_color = plt.cm.viridis((score - min(node_colors)) / (max(node_colors) - min(node_colors)))\n",
    "\n",
    "        ax.scatter(x, y, s=800, c=[fill_color], edgecolors='black', linewidths=1, zorder=5)\n",
    "\n",
    "        # Shortened label below node\n",
    "        text = textwrap.shorten(labels.get(node, \"\"), width=50, placeholder=\"...\")\n",
    "        ax.text(x, y - 30, text, ha='center', va='top', fontsize=8,\n",
    "                bbox=dict(boxstyle=\"round,pad=0.2\", fc=\"white\", ec=\"black\", lw=0.4), zorder=10)\n",
    "\n",
    "    # Colorbar\n",
    "    sm = plt.cm.ScalarMappable(cmap=plt.cm.viridis,\n",
    "                               norm=plt.Normalize(vmin=min(node_colors), vmax=max(node_colors)))\n",
    "    sm.set_array(node_colors)\n",
    "    fig.colorbar(sm, ax=ax, label=\"PRM Score\")\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Final Beam 1 ---\n",
      "Score: 1.31\n",
      "<|system|>\n",
      "You are a helpful assistant.\n",
      "<|user|>\n",
      "Roger has 5 tennis balls. He buys 2 cans of 3 tennis balls each. How many tennis balls does he have now?\n",
      "<|assistant|>\n",
      "Roger originally had 5 tennis balls. When he bought 2 cans, each containing 3 tennis balls, he received a total of 6 tennis balls from the cans (2 cans with 3 balls each). So, in total, Roger now has 5 tennis balls from before plus the 6 tennis balls from the cans, for a total of 11 tennis balls:\n",
      "\n",
      "Original tennis balls: 5\n",
      "Tennis balls from cans: 6 (2 cans with 3 balls each)\n",
      "Total tennis balls: 11\n",
      "\n",
      "So, Roger now has 11 tennis balls in total.\n",
      "\n",
      "Alternatively, you can also represent this using mathematical equations:\n",
      "\n",
      "Original tennis balls: 5\n",
      "Tennis balls from cans: 2 cans with 3 balls each = 6\n",
      "Total tennis balls: 5 + 6 = 11\n",
      "\n",
      "--- Final Beam 2 ---\n",
      "Score: -0.01\n",
      "<|system|>\n",
      "You are a helpful assistant.\n",
      "<|user|>\n",
      "Roger has 5 tennis balls. He buys 2 cans of 3 tennis balls each. How many tennis balls does he have now?\n",
      "<|assistant|>\n",
      "Roger originally had 5 tennis balls. When he bought 2 cans, each containing 3 tennis balls, he received a total of 6 tennis balls from the cans (2 cans with 3 balls each). So, in total, Roger now has 5 tennis balls from before plus the 6 tennis balls from the cans, for a total of 11 tennis balls:\n",
      "\n",
      "Original tennis balls: 5\n",
      "Tennis balls from cans: 6\n",
      "Total tennis balls: 11 (5 + 6)\n",
      "\n",
      "So, Roger now has 11 tennis balls in total.\n",
      "\n",
      "Alternatively, you could also calculate it by finding out how many tennis balls were in total before buying the cans (5) and then adding the number of tennis balls in the cans (6):\n",
      "\n",
      "Total tennis balls before buying cans: 5\n",
      "Tennis balls from cans:\n",
      "\n",
      "--- Final Beam 3 ---\n",
      "Score: -1.93\n",
      "<|system|>\n",
      "You are a helpful assistant.\n",
      "<|user|>\n",
      "Roger has 5 tennis balls. He buys 2 cans of 3 tennis balls each. How many tennis balls does he have now?\n",
      "<|assistant|>\n",
      "Roger originally had 5 tennis balls. When he bought 2 cans, each containing 3 tennis balls, he received a total of 6 tennis balls from the cans (2 cans with 3 balls each). So, in total, Roger now has 5 tennis balls from before plus the 6 tennis balls from the cans, for a total of 11 tennis balls:\n",
      "\n",
      "Original tennis balls: 5\n",
      "Tennis balls from cans: 6 (2 cans with 3 balls each)\n",
      "Total tennis balls: 11\n",
      "\n",
      "So, Roger now has 11 tennis balls in total.\n",
      "\n",
      "--- Final Beam 4 ---\n",
      "Score: -1.93\n",
      "<|system|>\n",
      "You are a helpful assistant.\n",
      "<|user|>\n",
      "Roger has 5 tennis balls. He buys 2 cans of 3 tennis balls each. How many tennis balls does he have now?\n",
      "<|assistant|>\n",
      "Roger originally had 5 tennis balls. When he bought 2 cans, each containing 3 tennis balls, he received a total of 6 tennis balls from the cans (2 cans with 3 balls each). So, in total, Roger now has 5 tennis balls from before plus the 6 tennis balls from the cans, for a total of 11 tennis balls:\n",
      "\n",
      "Original tennis balls: 5\n",
      "Tennis balls from cans: 6\n",
      "Total tennis balls: 11 (5 + 6)\n",
      "\n",
      "So, Roger now has 11 tennis balls in total.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Roger has 5 tennis balls. He buys 2 cans of 3 tennis balls each. How many tennis balls does he have now?\"\n",
    "beams, graph = beam_search_with_prm(prompt, reasoning_model, reasoning_tokenizer, reward_model, reward_tokenizer, N=4, M=2, max_steps=3)\n",
    "\n",
    "for i, (text, score, _) in enumerate(beams):\n",
    "    print(f\"--- Final Beam {i+1} ---\\nScore: {score:.2f}\\n{text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beam search tree visualization\n",
    "plot_trace_graph_tree_clean(graph)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reasoning_llm_scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
